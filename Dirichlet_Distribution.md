The [Dirichlet Distribution](https://en.wikipedia.org/wiki/Dirichlet_distribution) is a multivariate probability distribution that describes $\mathbf{k}>=2$ variables $\mathbf{X_1,X_2,...,X_k}$, such that each $\mathbf{x_i} \in (0,1)$, and $\sum_{i=1}^{N}x_i=1$, that is parametrized by a vector of positive-valued parameters $\alpha=(\alpha_1,\alpha_2,..,\alpha_k)$. The parameters *do not* have to be integers, they only need to be positive real numbers. They are not "normalized" in any way, they are parameters of this distribution.

The Dirichlet distribution is a generalization of the **beta distribution** into multiple dimensions, so you can start by learning about the beta distribution. Beta is univariate distribution of a random variable $\mathbf{X} \in (0,1)$ parameterized by parameters $\alpha$ and $\beta$. The nice intuition about it comes if you recall that it is a conjugate prior for the binomial distribution and if we assume a beta prior parameterized by $\alpha$ and $\beta$ for the binomial distribution's probability parameter $\mathbf{p}$, then the posterior distribution of $\mathbf{p}$ is also a beta distribution parameterized by $\alpha^{'}=\alpha + number of successes$ and $\beta^{'}=\beta+number of failures$. So you can think of $\alpha$ and $\beta$ as of *pseudocounts*(they do not need to be integers) of successes and failures.

In case of the Dirichlet Distribution, it is a conjugate prior of the multinomial distribution. If in the case of the binomial distribution we can think of it in terms of drawing and black balls with replacement from urn, then in case of the multinomial distribution we are drawing with replacement $\mathbf{N}$ balls appearing in $\mathbf{k}$ colors, where each of colors of the balls can be drawn with probabilities $p_1,p_2,...,p_k$. The Dirichlet Distribution is a conjugate prior of $p_1,p_2,...,p_k$ probabilities and $\alpha_1,\alpha_2,...,\alpha_k$ parameters can be thought as pesudocounts of balls of each color assumed a priori. In Dirichlet multinomial model $\alpha_1,\alpha_2,...,\alpha_k$ get updated by summing them with observed counts in each category: $\alpha_1 + n_1, ...,\alpha_k + n_k$ in similar fashion as in case of beta binomial model.

The higher value of $\alpha$ , the greater "weight" of $\mathbf{X_i}$ and the greater amount of the total "mass" is assigned to it. But the $\mathbf{X}$ must be satisfied the condition $x_1+...+x_k=1$. If all $\alpha_i$ are equal, the distribution is symmetric. If $\alpha_i<1$, it can be thought as anti-weight that pushes away $\x_i$ toward extremes, while when it is high, it attracts $\x_i$ toward some central value. if $\alpha_i = \alpha_j$, then the points are uniformly distributed. 
